import os
import time
import random
import base64
import re
from dataclasses import dataclass
from io import BytesIO
from typing import List, Dict, Any
from collections import Counter

import streamlit as st
import yaml
import pandas as pd

from openai import OpenAI
import google.generativeai as genai
import anthropic
from xai_sdk import Client as XAIClient
from xai_sdk.chat import user as xai_user, system as xai_system

import docx2txt
from PyPDF2 import PdfReader
from fpdf import FPDF

from pdf2image import convert_from_bytes
import pytesseract


# =========================
#  Localization
# =========================

UI_TEXT = {
    "en": {
        "app_title": "AuditFlow AI Â· Masterpiece Edition (FDA)",
        "subtitle": "FDA-oriented agentic document intelligence with painterly themes.",
        "tab_ocr_pdf": "OCR PDF Intelligence",
        "tab_file_transform": "File Transform & Deep Summary",
        "tab_file_intel": "File Intelligence",
        "tab_multi_file": "Multi-File Synthesis",
        "tab_smart_replace": "Smart Replace",
        "tab_note_keeper": "AI Note Keeper",
        "upload_label": "Upload a document (PDF, DOCX, TXT):",
        "output_format": "Transform file into:",
        "format_markdown": "Markdown (.md)",
        "format_pdf": "PDF (.pdf)",
        "run_summary": "Generate 2,000â€“3,000 word Masterpiece summary",
        "chat_with_file": "Chat with this file",
        "api_key_section": "API Keys (browser-only, never sent to any server except LLM provider)",
        "provider": "Provider",
        "model": "Model",
        "custom_prompt": "Custom system prompt",
        "max_tokens": "Max tokens",
        "temperature": "Temperature",
        "user_prompt": "Your question / instruction",
        "agent_select": "FDA Agent (from advanced_agents.yaml)",
    },
    "zh": {
        "app_title": "AuditFlow AI Â· å¤§å¸«å‚‘ä½œç‰ˆï¼ˆFDA å°ˆç”¨ï¼‰",
        "subtitle": "é¢å‘ FDA å ±è¦èˆ‡åˆè¦éœ€æ±‚çš„ä»£ç†å¼æ–‡ä»¶æ™ºæ…§ç³»çµ±ï¼Œçµåˆè—è¡“é¢¨æ ¼é«”é©—ã€‚",
        "tab_ocr_pdf": "OCR æƒæ PDF æ™ºèƒ½åˆ†æ",
        "tab_file_transform": "æª”æ¡ˆè½‰æ›èˆ‡æ·±åº¦æ‘˜è¦",
        "tab_file_intel": "å–®ä¸€æ–‡ä»¶åˆ†æ",
        "tab_multi_file": "å¤šæ–‡ä»¶ç¶œåˆåˆ†æ",
        "tab_smart_replace": "æ™ºæ…§ç¯„æœ¬å¡«å¯«",
        "tab_note_keeper": "AI ç­†è¨˜ç®¡ç†å“¡",
        "upload_label": "ä¸Šå‚³æ–‡ä»¶ï¼ˆPDFã€DOCXã€TXTï¼‰ï¼š",
        "output_format": "å°‡æª”æ¡ˆè½‰æ›ç‚ºï¼š",
        "format_markdown": "Markdown (.md)",
        "format_pdf": "PDF (.pdf)",
        "run_summary": "ç”¢ç”Ÿ 2,000â€“3,000 å­—æ·±åº¦æ‘˜è¦ï¼ˆMarkdownï¼‰",
        "chat_with_file": "é‡å°æ­¤æ–‡ä»¶ç™¼å•",
        "api_key_section": "API é‡‘é‘°ï¼ˆåƒ…åœ¨æœ¬æ©Ÿç€è¦½å™¨ä¸­ä½¿ç”¨ï¼Œåƒ…é€å¾€ LLM ä¾›æ‡‰å•†ï¼‰",
        "provider": "æœå‹™æä¾›è€…",
        "model": "æ¨¡å‹",
        "custom_prompt": "è‡ªè¨‚ç³»çµ±æç¤ºï¼ˆSystem Promptï¼‰",
        "max_tokens": "æœ€å¤§ Token æ•¸",
        "temperature": "æº«åº¦",
        "user_prompt": "ä½ çš„å•é¡Œ / æŒ‡ä»¤",
        "agent_select": "FDA ä»£ç†äººï¼ˆä¾†è‡ª advanced_agents.yamlï¼‰",
    },
}


def t(key: str) -> str:
    lang = st.session_state.get("ui_lang", "en")
    return UI_TEXT.get(lang, UI_TEXT["en"]).get(key, key)


# =========================
#  Painter Styles
# =========================

@dataclass
class ArtistStyle:
    key: str
    display_name: str
    painter: str
    bg_gradient_light: str
    bg_gradient_dark: str
    panel_bg_rgba: str
    accent_color: str
    accent_soft: str
    font_family: str


ARTIST_STYLES: List[ArtistStyle] = [
    # (same 20 styles as before, unchanged)
    ArtistStyle(
        key="van_gogh",
        display_name="Starry Night",
        painter="Vincent van Gogh",
        bg_gradient_light="linear-gradient(135deg,#fdfbfb 0%,#ebedee 100%)",
        bg_gradient_dark="linear-gradient(135deg,#0f172a 0%,#1e293b 100%)",
        panel_bg_rgba="rgba(15, 23, 42, 0.75)",
        accent_color="#facc15",
        accent_soft="#fef9c3",
        font_family="'DM Sans', system-ui, -apple-system, BlinkMacSystemFont, sans-serif",
    ),
    # ... (Monet, Picasso, etc. â€“ omit here for brevity, keep exactly as previous app.py)
]

# For brevity, include all ARTIST_STYLES from previous version here.


def apply_theme(style: ArtistStyle, dark_mode: bool):
    bg = style.bg_gradient_dark if dark_mode else style.bg_gradient_light
    panel = style.panel_bg_rgba
    text_color = "#e5e7eb" if dark_mode else "#020617"

    css = f"""
    <style>
    html, body, [data-testid="stAppViewContainer"] {{
        background: {bg} !important;
        background-attachment: fixed;
        font-family: {style.font_family};
        color: {text_color};
    }}
    .glass-panel {{
        background: {panel};
        backdrop-filter: blur(18px);
        -webkit-backdrop-filter: blur(18px);
        border-radius: 20px;
        border: 1px solid rgba(255,255,255,0.18);
        padding: 1.25rem 1.5rem;
        margin-bottom: 1.5rem;
    }}
    .accent-title {{
        color: {style.accent_color};
    }}
    .accent-chip {{
        background: {style.accent_soft};
        color: #111827;
        border-radius: 9999px;
        padding: 0.15rem 0.7rem;
        font-size: 0.75rem;
        font-weight: 500;
        display: inline-flex;
        align-items: center;
        gap: 0.25rem;
    }}
    textarea, .stTextInput > div > div > input {{
        background: rgba(15,23,42,0.75) !important;
        color: #e5e7eb !important;
    }}
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)


def style_selector_ui() -> ArtistStyle:
    st.markdown("### ğŸ¨ Masterpiece Style Jackpot")
    style_keys = [s.key for s in ARTIST_STYLES]
    current_style_key = st.session_state.get("artist_style_key", style_keys[0])

    col1, col2 = st.columns([3, 1])
    with col1:
        selected_key = st.selectbox(
            "Style",
            options=style_keys,
            index=style_keys.index(current_style_key) if current_style_key in style_keys else 0,
            format_func=lambda k: next(s.display_name for s in ARTIST_STYLES if s.key == k),
            key="artist_style_dropdown",
        )
    with col2:
        if st.button("Inspire Me (Jackpot)"):
            placeholder = st.empty()
            for _ in range(15):
                rand_key = random.choice(style_keys)
                st.session_state.artist_style_key = rand_key
                placeholder.write(
                    f"ğŸ° ğŸ¨ {next(s.display_name for s in ARTIST_STYLES if s.key == rand_key)}"
                )
                time.sleep(0.06)
            placeholder.empty()

    st.session_state.artist_style_key = st.session_state.get("artist_style_key", selected_key)
    active_style = next(s for s in ARTIST_STYLES if s.key == st.session_state.artist_style_key)
    return active_style


# =========================
#  Agents (from YAML)
# =========================

def load_agents(path: str = "advanced_agents.yaml") -> List[Dict[str, Any]]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        return data.get("agents", [])
    except Exception as e:
        st.sidebar.error(f"Failed to load agents YAML: {e}")
        return []


def agent_selector_ui(agents: List[Dict[str, Any]]) -> Dict[str, Any]:
    if not agents:
        st.sidebar.warning("No agents loaded from advanced_agents.yaml.")
        return {}

    st.sidebar.markdown(f"### ğŸ¤– {t('agent_select')}")
    ids = [a["id"] for a in agents]

    def label_func(agent_id: str) -> str:
        a = next(ag for ag in agents if ag["id"] == agent_id)
        return a.get("display_name_zh", agent_id)

    default_idx = 0
    if "selected_agent_id" in st.session_state:
        try:
            default_idx = ids.index(st.session_state["selected_agent_id"])
        except ValueError:
            default_idx = 0

    selected_id = st.sidebar.selectbox(
        "Agent",
        options=ids,
        index=default_idx,
        format_func=label_func,
        key="agent_selectbox",
    )
    selected_agent = next(a for a in agents if a["id"] == selected_id)

    if st.session_state.get("selected_agent_id") != selected_id:
        st.session_state["selected_agent_id"] = selected_id
        st.session_state["llm_provider"] = selected_agent.get("default_provider", "Gemini")
        st.session_state["llm_model_id"] = selected_agent.get("default_model", "gemini-3-flash")
        st.session_state["llm_max_tokens"] = selected_agent.get("default_max_tokens", 4096)
        st.session_state["llm_temperature"] = selected_agent.get("default_temperature", 0.3)
        st.session_state["llm_system_prompt"] = selected_agent.get(
            "system_prompt_zh",
            "ä½ æ˜¯ä¸€ä½ FDA æ³•è¦åˆè¦èˆ‡ç­–ç•¥åˆ†æå°ˆå®¶ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚",
        )

    return selected_agent


# =========================
#  API Keys
# =========================

def render_api_key_inputs():
    st.sidebar.markdown(f"### ğŸ” {t('api_key_section')}")
    with st.sidebar.expander("OpenAI", expanded=False):
        env_val = os.getenv("OPENAI_API_KEY")
        if env_val:
            st.markdown("Using environment OpenAI API keyï¼ˆä¸é¡¯ç¤ºå¯¦éš›å€¼ï¼‰ã€‚")
            st.session_state["openai_api_key"] = env_val
        else:
            st.session_state["openai_api_key"] = st.text_input(
                "OpenAI API Key",
                type="password",
                value=st.session_state.get("openai_api_key", ""),
            )

    with st.sidebar.expander("Gemini", expanded=False):
        env_val = os.getenv("GEMINI_API_KEY")
        if env_val:
            st.markdown("Using environment Gemini API keyï¼ˆä¸é¡¯ç¤ºå¯¦éš›å€¼ï¼‰ã€‚")
            st.session_state["gemini_api_key"] = env_val
        else:
            st.session_state["gemini_api_key"] = st.text_input(
                "Gemini API Key",
                type="password",
                value=st.session_state.get("gemini_api_key", ""),
            )

    with st.sidebar.expander("Anthropic", expanded=False):
        env_val = os.getenv("ANTHROPIC_API_KEY")
        if env_val:
            st.markdown("Using environment Anthropic API keyï¼ˆä¸é¡¯ç¤ºå¯¦éš›å€¼ï¼‰ã€‚")
            st.session_state["anthropic_api_key"] = env_val
        else:
            st.session_state["anthropic_api_key"] = st.text_input(
                "Anthropic API Key",
                type="password",
                value=st.session_state.get("anthropic_api_key", ""),
            )

    with st.sidebar.expander("XAI (Grok)", expanded=False):
        env_val = os.getenv("XAI_API_KEY")
        if env_val:
            st.markdown("Using environment XAI API keyï¼ˆä¸é¡¯ç¤ºå¯¦éš›å€¼ï¼‰ã€‚")
            st.session_state["xai_api_key"] = env_val
        else:
            st.session_state["xai_api_key"] = st.text_input(
                "XAI API Key",
                type="password",
                value=st.session_state.get("xai_api_key", ""),
            )


# =========================
#  Model & Prompt Controls (Global)
# =========================

MODEL_CATALOG = {
    "OpenAI": [
        {"id": "gpt-4o-mini", "label": "GPTâ€‘4o mini"},
        {"id": "gpt-4.1-mini", "label": "GPTâ€‘4.1 mini"},
    ],
    "Gemini": [
        {"id": "gemini-2.5-flash", "label": "Gemini 2.5 Flash"},
        {"id": "gemini-3-flash", "label": "Gemini 3 Flash"},
    ],
    "Anthropic": [
        {"id": "claude-3.5-sonnet", "label": "Claude 3.5 Sonnet"},
        {"id": "claude-3.5-haiku", "label": "Claude 3.5 Haiku"},
    ],
    "XAI (Grok)": [
        {"id": "grok-4", "label": "Grok-4 (XAI)"},
    ],
}


def render_llm_controls():
    st.sidebar.markdown("### ğŸ§  LLM & Prompt")
    provider = st.sidebar.selectbox(
        t("provider"),
        list(MODEL_CATALOG.keys()),
        index=list(MODEL_CATALOG.keys()).index(st.session_state.get("llm_provider", "Gemini")),
        key="llm_provider",
    )
    models = MODEL_CATALOG[provider]
    model_ids = [m["id"] for m in models]

    default_model = st.session_state.get("llm_model_id", model_ids[0])
    if default_model not in model_ids:
        default_model = model_ids[0]

    model_id = st.sidebar.selectbox(
        t("model"),
        options=model_ids,
        index=model_ids.index(default_model),
        format_func=lambda m: next(x["label"] for x in models if x["id"] == m),
        key="llm_model_id",
    )

    max_tokens = st.sidebar.slider(
        t("max_tokens"), min_value=256, max_value=8192,
        value=int(st.session_state.get("llm_max_tokens", 4096)), step=256,
        key="llm_max_tokens",
    )
    temperature = st.sidebar.slider(
        t("temperature"),
        min_value=0.0,
        max_value=1.5,
        value=float(st.session_state.get("llm_temperature", 0.3)),
        step=0.05,
        key="llm_temperature",
    )
    system_prompt = st.sidebar.text_area(
        t("custom_prompt"),
        value=st.session_state.get("llm_system_prompt", ""),
        key="llm_system_prompt",
        height=180,
    )
    return provider, model_id, max_tokens, temperature, system_prompt


def get_llm_config():
    return (
        st.session_state.get("llm_provider", "Gemini"),
        st.session_state.get("llm_model_id", "gemini-3-flash"),
        int(st.session_state.get("llm_max_tokens", 4096)),
        float(st.session_state.get("llm_temperature", 0.3)),
        st.session_state.get("llm_system_prompt", "ä½ æ˜¯ä¸€ä½ FDA æ³•è¦åˆè¦èˆ‡ç­–ç•¥åˆ†æå°ˆå®¶ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
    )


# =========================
#  LLM Call Wrapper (text-only)
# =========================

def call_llm(
    provider: str,
    model: str,
    system_prompt: str,
    user_messages: List[Dict[str, str]],
    max_tokens: int = 2048,
    temperature: float = 0.4,
) -> str:
    if provider == "OpenAI":
        api_key = st.session_state.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("OpenAI API key is required.")
            return ""
        client = OpenAI(api_key=api_key)
        messages = [{"role": "system", "content": system_prompt}] + user_messages
        resp = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return resp.choices[0].message.content

    elif provider == "Gemini":
        api_key = st.session_state.get("gemini_api_key") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            st.error("Gemini API key is required.")
            return ""
        genai.configure(api_key=api_key)
        model_obj = genai.GenerativeModel(model)
        full_prompt = f"{system_prompt}\n\n" + "\n\n".join(
            f"{m['role'].upper()}: {m['content']}" for m in user_messages
        )
        resp = model_obj.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
            ),
        )
        return resp.text

    elif provider == "Anthropic":
        api_key = st.session_state.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            st.error("Anthropic API key is required.")
            return ""
        client = anthropic.Anthropic(api_key=api_key)
        messages = [m for m in user_messages if m["role"] != "system"]
        resp = client.messages.create(
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            system=system_prompt,
            messages=[{"role": m["role"], "content": m["content"]} for m in messages],
        )
        return "".join(block.text for block in resp.content if hasattr(block, "text"))

    elif provider == "XAI (Grok)":
        api_key = st.session_state.get("xai_api_key") or os.getenv("XAI_API_KEY")
        if not api_key:
            st.error("XAI API key is required.")
            return ""
        client = XAIClient(api_key=api_key, timeout=3600)
        chat = client.chat.create(model=model)
        chat.append(xai_system(system_prompt))
        for m in user_messages:
            if m["role"] == "user":
                chat.append(xai_user(m["content"]))
        response = chat.sample()
        return response.content

    else:
        st.error("Unsupported provider.")
        return ""


# =========================
#  File Utilities
# =========================

def extract_text_from_pdf(file_bytes: BytesIO) -> str:
    reader = PdfReader(file_bytes)
    texts = []
    for page in reader.pages:
        texts.append(page.extract_text() or "")
    return "\n".join(texts)


def extract_text_from_docx(file_bytes: BytesIO) -> str:
    return docx2txt.process(file_bytes)


def extract_text_from_txt(file_bytes: BytesIO) -> str:
    return file_bytes.read().decode("utf-8", errors="ignore")


def extract_text(uploaded_file) -> str:
    name = uploaded_file.name.lower()
    data = BytesIO(uploaded_file.read())
    if name.endswith(".pdf"):
        return extract_text_from_pdf(data)
    elif name.endswith(".docx"):
        return extract_text_from_docx(data)
    elif name.endswith(".txt"):
        return extract_text_from_txt(data)
    elif name.endswith(".md"):
        return data.read().decode("utf-8", errors="ignore")
    else:
        st.error("Unsupported format. Please upload PDF, DOCX, TXT, or MD.")
        return ""


def markdown_to_pdf_bytes(md_text: str) -> bytes:
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=11)
    for line in md_text.splitlines():
        pdf.multi_cell(0, 5, line)
    pdf_bytes = BytesIO()
    pdf.output(pdf_bytes)
    pdf_bytes.seek(0)
    return pdf_bytes.getvalue()


# =========================
#  Summary Prompt (Deep)
# =========================

def build_deep_summary_prompt(doc_text: str, lang: str) -> str:
    if lang == "en":
        language_instruction = "Write the entire output in English."
    else:
        language_instruction = "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«æ•´ä»½è¼¸å‡ºï¼Œä¸¦ä»¥ FDA å¯©æŸ¥èˆ‡åˆè¦è¦–è§’é€²è¡Œæ·±å…¥åˆ†æã€‚"

    base = f"""
ä½ æ˜¯ä¸€ä½å…·å‚™ FDA è¦ç¯„ã€é†«è—¥/é†«æå¯©æŸ¥èˆ‡æˆ°ç•¥è¦åŠƒå°ˆé•·çš„ã€Œé«˜éšç­–ç•¥å¯©é–±å®˜ã€èˆ‡ã€ŒçŸ¥è­˜æ¶æ§‹å¸«ã€ã€‚
{language_instruction}

ä½ å°‡æ”¶åˆ°ä¸€ä»½æ–‡ä»¶å…§å®¹ã€‚è«‹ä¾å‰è¿°è¦æ ¼ç”¢å‡º 2,000â€“3,000 å­—çš„æ·±åº¦ Markdown å ±å‘Šã€‚

[DOCUMENT START]
{doc_text[:100000]}
[DOCUMENT END]
"""
    return base.strip()


# =========================
#  OCR Helper Functions
# =========================

def preview_pdf(pdf_bytes: bytes):
    b64 = base64.b64encode(pdf_bytes).decode("utf-8")
    pdf_display = f"""
    <iframe src="data:application/pdf;base64,{b64}" width="100%" height="600" type="application/pdf"></iframe>
    """
    st.markdown(pdf_display, unsafe_allow_html=True)


def run_local_ocr(pdf_bytes: bytes, pages: List[int], lang_choice: str) -> str:
    if not pages:
        return ""

    if lang_choice == "English":
        lang = "eng"
    elif lang_choice == "ç¹é«”ä¸­æ–‡":
        lang = "chi_tra"
    else:
        lang = "eng+chi_tra"

    texts = []
    for p in pages:
        images = convert_from_bytes(pdf_bytes, dpi=200, first_page=p, last_page=p)
        if not images:
            continue
        img = images[0]
        page_text = pytesseract.image_to_string(img, lang=lang)
        texts.append(f"=== Page {p} ===\n{page_text.strip()}")
    return "\n\n".join(texts)


def run_llm_ocr(pdf_bytes: bytes, pages: List[int], model_choice: str) -> str:
    if not pages:
        return ""

    texts = []
    for p in pages:
        images = convert_from_bytes(pdf_bytes, dpi=200, first_page=p, last_page=p)
        if not images:
            continue
        img = images[0]
        buffered = BytesIO()
        img.save(buffered, format="PNG")
        img_bytes = buffered.getvalue()

        if model_choice in ["gemini-3-flash", "gemini-2.5-flash"]:
            api_key = st.session_state.get("gemini_api_key") or os.getenv("GEMINI_API_KEY")
            if not api_key:
                st.error("Gemini API key is required for LLM OCR.")
                return ""
            genai.configure(api_key=api_key)
            model_obj = genai.GenerativeModel(model_choice)
            prompt = "Please perform OCR on this page and return only the plain text, preserving reading order."
            resp = model_obj.generate_content(
                [prompt, {"mime_type": "image/png", "data": img_bytes}],
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=2048,
                    temperature=0.0,
                ),
            )
            page_text = resp.text or ""
            texts.append(f"=== Page {p} ===\n{page_text.strip()}")

        elif model_choice == "gpt-4o-mini":
            api_key = st.session_state.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            if not api_key:
                st.error("OpenAI API key is required for LLM OCR.")
                return ""
            client = OpenAI(api_key=api_key)
            b64_img = base64.b64encode(img_bytes).decode("utf-8")
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "Extract all legible text from this image. Output plain text only.",
                        },
                        {
                            "type": "input_image",
                            "image_url": {
                                "url": f"data:image/png;base64,{b64_img}"
                            },
                        },
                    ],
                }
            ]
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=2048,
            )
            page_text = resp.choices[0].message.content or ""
            texts.append(f"=== Page {p} ===\n{page_text.strip()}")
        else:
            st.error("Unsupported model for LLM OCR.")
            return ""

    return "\n\n".join(texts)


def build_word_freq_chart(text: str):
    tokens = re.findall(r"[A-Za-z\u4e00-\u9fff]+", text.lower())
    stopwords = {
        "the", "and", "of", "to", "in", "for", "a", "is", "on", "with", "that",
        "this", "by", "or", "as", "an", "be", "are", "at", "from"
    }
    tokens = [t for t in tokens if t not in stopwords and len(t) > 1]
    if not tokens:
        return
    counter = Counter(tokens)
    top = counter.most_common(20)
    if not top:
        return
    df = pd.DataFrame(top, columns=["word", "count"]).set_index("word")
    st.markdown("#### ğŸ”  Word Frequency Graph (Top Terms)")
    st.bar_chart(df)


def build_ocr_summary_prompt(ocr_text: str, lang: str) -> str:
    language_instruction = (
        "Write the entire output in English."
        if lang == "en"
        else "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¸¦ä»¥ FDA/å°ˆæ¥­å¯©æŸ¥è§€é»é€²è¡Œæ•´ç†ã€‚"
    )
    base = f"""
ä½ å°‡æ”¶åˆ°ä¸€æ®µç”± OCR æ“·å–çš„æ–‡ä»¶å…§å®¹ï¼ˆå¯èƒ½æ“æœ‰å™ªéŸ³æˆ–æ‹¼å­—éŒ¯èª¤ï¼‰ã€‚è«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š

1. æ•´ç†ä¸¦ä¿®æ­£å¯æ˜é¡¯è¾¨è­˜çš„æ–‡å­—éŒ¯èª¤ï¼ˆä½†é¿å…æ†‘ç©ºè£œé€ æ•¸æ“šï¼‰ã€‚
2. ç”¢å‡ºä¸€ä»½çµæ§‹åŒ– Markdown æ‘˜è¦ï¼Œè‡³å°‘åŒ…å«ï¼š
   - æ–‡ä»¶æ•´é«”ç›®çš„èˆ‡ä¸»é¡Œ
   - ä¸»è¦é‡é»èˆ‡è«–é»
   - é—œéµé¢¨éšªæˆ–éœ€é—œæ³¨è­°é¡Œ
3. ç”¢å‡ºä¸€æ®µã€Œé—œéµè©é—œè¯æ¦‚è§€ã€ï¼ˆWord Graph æ¦‚è¦æ•˜è¿°ï¼‰ï¼š
   - ä»¥æ–‡å­—æ–¹å¼æè¿°ä¸»è¦é—œéµè©ç¾¤èˆ‡å…¶å½¼æ­¤é—œä¿‚ã€èšé¡æˆ–ä¸»é¡Œã€‚
4. èƒå– **20 å€‹æœ€é‡è¦çš„å¯¦é«”**ï¼ˆå¦‚è—¥å“åç¨±ã€æ©Ÿæ§‹ã€é—œéµæŠ€è¡“åè©ã€è©¦é©—ä»£ç¢¼ç­‰ï¼‰ï¼Œ
   ä¸¦ä»¥ Markdown è¡¨æ ¼è¼¸å‡ºï¼Œæ¬„ä½åŒ…å«ï¼š
   - #ï¼ˆåºè™Ÿï¼‰
   - Entityï¼ˆå¯¦é«”åç¨±ï¼‰
   - Typeï¼ˆå¯¦é«”é¡å‹ï¼‰
   - Context Snippetï¼ˆé—œéµä¸Šä¸‹æ–‡æ‘˜éŒ„ï¼‰
   - Relevanceï¼ˆç‚ºä½•é‡è¦ï¼‰

{language_instruction}

[OCR TEXT START]
{ocr_text[:80000]}
[OCR TEXT END]
"""
    return base.strip()


# =========================
#  Limited Model Selector for OCR / Notes Q&A
# =========================

LIMITED_QA_MODELS = {
    "Gemini 3 Flash": ("Gemini", "gemini-3-flash"),
    "Gemini 2.5 Flash": ("Gemini", "gemini-2.5-flash"),
    "GPTâ€‘4o mini": ("OpenAI", "gpt-4o-mini"),
}


def limited_model_selector(default_label: str = "Gemini 3 Flash"):
    labels = list(LIMITED_QA_MODELS.keys())
    if default_label not in labels:
        default_label = labels[0]
    label = st.selectbox("é¸æ“‡æ¨¡å‹", labels, index=labels.index(default_label))
    provider, model_id = LIMITED_QA_MODELS[label]
    max_tokens = st.number_input(
        "æœ€å¤§ tokensï¼ˆå»ºè­° â‰¤ 12000ï¼‰",
        min_value=512,
        max_value=16000,
        value=12000,
        step=512,
    )
    temperature = st.slider("æº«åº¦", 0.0, 1.5, 0.4, 0.05)
    return provider, model_id, int(max_tokens), float(temperature)


# =========================
#  Tabs
# =========================

def tab_ocr_pdf_intelligence():
    st.markdown(f"## {t('tab_ocr_pdf')}")
    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    # Upload & manage PDF
    uploaded = st.file_uploader(
        "ä¸Šå‚³è¦é€²è¡Œ OCR çš„ PDFï¼ˆæƒææˆ–å«å½±åƒï¼‰ï¼š",
        type=["pdf"],
        key="ocr_pdf_uploader",
    )

    col_up1, col_up2 = st.columns([3, 1])
    with col_up1:
        if uploaded is not None:
            # Save bytes to session
            pdf_bytes = uploaded.read()
            st.session_state["ocr_pdf_bytes"] = pdf_bytes
            st.session_state["ocr_pdf_name"] = uploaded.name
    with col_up2:
        if st.button("æ¸…é™¤ç›®å‰ PDF"):
            st.session_state.pop("ocr_pdf_bytes", None)
            st.session_state.pop("ocr_pdf_name", None)
            st.session_state.pop("ocr_text", None)

    pdf_bytes = st.session_state.get("ocr_pdf_bytes")
    if pdf_bytes:
        st.markdown(f"**ç›®å‰ PDFï¼š** {st.session_state.get('ocr_pdf_name','')}")
        preview_pdf(pdf_bytes)

        # Page selection
        reader = PdfReader(BytesIO(pdf_bytes))
        num_pages = len(reader.pages)
        st.markdown(f"æ­¤æª”å…±æœ‰ **{num_pages}** é ã€‚")
        page_nums = list(range(1, num_pages + 1))
        selected_pages = st.multiselect(
            "é¸æ“‡è¦é€²è¡Œ OCR çš„é æ•¸",
            options=page_nums,
            default=page_nums,
        )

        # OCR method
        ocr_method = st.radio(
            "OCR æ–¹å¼",
            ["æœ¬åœ° OCR (pdf2image + pytesseract)", "LLM OCR (Gemini / GPTâ€‘4o-mini)"],
            horizontal=True,
        )

        if ocr_method.startswith("æœ¬åœ°"):
            ocr_lang = st.selectbox(
                "OCR èªè¨€",
                ["English", "ç¹é«”ä¸­æ–‡", "ä¸­è‹±æ··åˆ"],
                index=2,
            )
            if st.button("åŸ·è¡Œæœ¬åœ° OCR"):
                with st.spinner("Running local OCR (pdf2image + pytesseract)â€¦"):
                    text = run_local_ocr(pdf_bytes, selected_pages, ocr_lang)
                    if not text.strip():
                        st.warning("æœªæ“·å–åˆ°æ–‡å­—ï¼Œè«‹ç¢ºèªé é¢æ˜¯å¦ç‚ºå½±åƒæˆ–å˜—è©¦ä¸åŒèªè¨€è¨­å®šã€‚")
                    else:
                        st.session_state["ocr_text"] = text

        else:
            llm_ocr_model = st.selectbox(
                "é¸æ“‡ LLM æ¨¡å‹ç”¨æ–¼ OCR",
                ["gemini-3-flash", "gemini-2.5-flash", "gpt-4o-mini"],
                index=0,
            )
            if st.button("åŸ·è¡Œ LLM OCR"):
                with st.spinner("Running LLM-based OCR on selected pagesâ€¦"):
                    text = run_llm_ocr(pdf_bytes, selected_pages, llm_ocr_model)
                    if not text.strip():
                        st.warning("LLM OCR æœªæ“·å–åˆ°æ–‡å­—ï¼Œè«‹æª¢æŸ¥ API Key æˆ–å˜—è©¦ä¸åŒæ¨¡å‹ã€‚")
                    else:
                        st.session_state["ocr_text"] = text

    # OCR Text Editing & Summary
    if "ocr_text" in st.session_state and st.session_state["ocr_text"]:
        st.markdown("---")
        st.markdown("### âœï¸ OCR çµæœç·¨è¼¯")
        view_mode = st.radio("æª¢è¦–æ¨¡å¼", ["Markdown é è¦½", "ç´”æ–‡å­—"], horizontal=True)
        ocr_text = st.text_area(
            "å¯ç·¨è¼¯ OCR æ–‡æœ¬ï¼ˆå¯è¦–ç‚º Markdown æˆ–ç´”æ–‡å­—ï¼‰",
            value=st.session_state["ocr_text"],
            height=260,
            key="ocr_text_edit",
        )
        st.session_state["ocr_text"] = ocr_text

        if view_mode == "Markdown é è¦½":
            st.markdown("#### é è¦½")
            st.markdown(ocr_text)
        else:
            st.markdown("#### ç´”æ–‡å­—é¡¯ç¤º")
            st.text(ocr_text[:5000])

        if st.button("ç”¢ç”Ÿ OCR æ–‡ä»¶æ‘˜è¦ + Word Graph + 20 å¯¦é«”è¡¨"):
            lang = st.session_state.get("ui_lang", "zh")
            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
            prompt = build_ocr_summary_prompt(ocr_text, lang)
            with st.spinner("Generating OCR-based summary and entitiesâ€¦"):
                summary = call_llm(
                    provider=provider,
                    model=model_id,
                    system_prompt=system_prompt,
                    user_messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
            st.session_state["ocr_summary_md"] = summary or ""

        if "ocr_summary_md" in st.session_state and st.session_state["ocr_summary_md"]:
            st.markdown("### ğŸ“„ OCR æ–‡ä»¶ç¸½çµ")
            summary_view = st.radio(
                "ç¸½çµæª¢è¦–æ¨¡å¼",
                ["Markdown", "ç´”æ–‡å­—"],
                horizontal=True,
                key="ocr_summary_view_mode",
            )
            if summary_view == "Markdown":
                st.markdown(st.session_state["ocr_summary_md"])
            else:
                st.text(st.session_state["ocr_summary_md"])

            # Word frequency graph from cleaned OCR text
            build_word_freq_chart(st.session_state["ocr_text"])

            # Q&A on OCR doc
            st.markdown("---")
            st.markdown("### ğŸ’¬ é‡å° OCR æ–‡ä»¶æŒçºŒæå•")
            qa_question = st.text_area("ä½ çš„æå• / ä»»å‹™æè¿°", key="ocr_qa_question")
            provider_q, model_q, max_tokens_q, temp_q = limited_model_selector("Gemini 3 Flash")
            answer_view = st.radio(
                "å›ç­”é¡¯ç¤ºç‚º",
                ["Markdown", "ç´”æ–‡å­—"],
                horizontal=True,
                key="ocr_qa_answer_view",
            )
            if st.button("å‘æ¨¡å‹æå•ï¼ˆOCR æ–‡ä»¶ç‚ºèƒŒæ™¯ï¼‰"):
                if not qa_question.strip():
                    st.warning("è«‹è¼¸å…¥å•é¡Œã€‚")
                else:
                    context = f"""
ä»¥ä¸‹ç‚º OCR å¾Œä¸¦å¯ç·¨è¼¯ä¹‹æ–‡ä»¶å…§å®¹ï¼ˆå¯èƒ½ä»å«å°‘é‡å™ªéŸ³ï¼‰ï¼š

[OCR TEXT]
{st.session_state['ocr_text'][:80000]}

è‹¥æœ‰å¯ç”¨çš„æ‘˜è¦ï¼Œå‰‡å¦‚ä¸‹ï¼š

[SUMMARY]
{st.session_state.get('ocr_summary_md','')[:40000]}
"""
                    with st.spinner("Model thinking with OCR documentâ€¦"):
                        answer = call_llm(
                            provider=provider_q,
                            model=model_q,
                            system_prompt="ä½ æ˜¯ä¸€ä½å°ˆæ¥­æ–‡ä»¶å¯©é–±èˆ‡èªªæ˜å°ˆå®¶ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æˆ–è‹±æ–‡ï¼ˆä¾å…§å®¹è€Œå®šï¼‰æ¸…æ¥šå›ç­”ã€‚",
                            user_messages=[
                                {"role": "user", "content": context},
                                {"role": "user", "content": qa_question},
                            ],
                            max_tokens=max_tokens_q,
                            temperature=temp_q,
                        )
                    if answer_view == "Markdown":
                        st.markdown(answer or "_No answer produced._")
                    else:
                        st.text(answer or "_No answer produced._")

    st.markdown("</div>", unsafe_allow_html=True)


def tab_file_transform_deep_summary():
    st.markdown(f"## {t('tab_file_transform')}")
    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    uploaded = st.file_uploader(
        t("upload_label"),
        type=["pdf", "docx", "txt"],
        key="file_transform_uploader",
    )

    output_format = st.radio(
        t("output_format"),
        [t("format_markdown"), t("format_pdf")],
        horizontal=True,
        key="output_format_choice",
    )

    if uploaded is not None:
        if st.button(t("run_summary"), type="primary"):
            with st.spinner("Extracting text and generating deep summaryâ€¦"):
                raw_text = extract_text(uploaded)
                if not raw_text.strip():
                    st.error("No readable text extracted from the file.")
                    st.markdown("</div>", unsafe_allow_html=True)
                    return

                provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
                lang = st.session_state.get("ui_lang", "zh")

                prompt = build_deep_summary_prompt(raw_text, lang)
                output = call_llm(
                    provider=provider,
                    model=model_id,
                    system_prompt=system_prompt,
                    user_messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
                if not output:
                    st.markdown("</div>", unsafe_allow_html=True)
                    return

                st.session_state["latest_file_text"] = raw_text
                st.session_state["latest_file_summary_md"] = output
                st.session_state["latest_file_name"] = uploaded.name

                st.markdown("### ğŸ“„ Deep Summary (Markdown)")
                st.markdown(output)

                if output_format == t("format_markdown"):
                    st.download_button(
                        "Download Markdown",
                        data=output.encode("utf-8"),
                        file_name=f"{uploaded.name}.summary.md",
                        mime="text/markdown",
                    )
                else:
                    pdf_bytes = markdown_to_pdf_bytes(output)
                    st.download_button(
                        "Download PDF",
                        data=pdf_bytes,
                        file_name=f"{uploaded.name}.summary.pdf",
                        mime="application/pdf",
                    )

    if "latest_file_text" in st.session_state:
        st.markdown("---")
        st.markdown(f"### ğŸ’¬ {t('chat_with_file')} â€” {st.session_state.get('latest_file_name', '')}")
        user_q = st.text_area(t("user_prompt"), key="file_chat_prompt")
        if st.button("Ask the file"):
            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
            full_context = f"""
ä»¥ä¸‹æ˜¯åŸå§‹æ–‡ä»¶å…§å®¹èˆ‡è©²æ–‡ä»¶ä¹‹é•·ç¯‡æ‘˜è¦ã€‚è«‹åš´æ ¼æ ¹æ“šæ­¤ç­‰è³‡è¨Šä½œç­”ï¼Œè‹¥å…§å®¹ä¸è¶³ä»¥æ”¯æŒç­”æ¡ˆï¼Œè«‹æ˜ç¢ºèªªæ˜ã€Œæ–‡ä»¶æœªæä¾›è¶³å¤ è³‡è¨Šã€ã€‚

[ORIGINAL DOCUMENT]
{st.session_state['latest_file_text'][:60000]}

[SUMMARY]
{st.session_state['latest_file_summary_md'][:40000]}
"""
            question = user_q.strip()
            if not question:
                st.warning("è«‹è¼¸å…¥å•é¡Œã€‚")
            else:
                with st.spinner("Thinking with the documentâ€¦"):
                    answer = call_llm(
                        provider=provider,
                        model=model_id,
                        system_prompt=system_prompt,
                        user_messages=[
                            {"role": "user", "content": full_context},
                            {"role": "user", "content": question},
                        ],
                        max_tokens=max_tokens,
                        temperature=temperature,
                    )
                st.markdown("#### Answer")
                st.markdown(answer or "_No answer produced._")

    st.markdown("</div>", unsafe_allow_html=True)


def tab_file_intelligence():
    st.markdown(f"## {t('tab_file_intel')}")
    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)
    up = st.file_uploader(
        t("upload_label"),
        type=["pdf", "docx", "txt", "md"],
        key="file_intel_uploader",
    )
    if up is not None and st.button("Analyze File"):
        with st.spinner("Analyzing fileâ€¦"):
            text = extract_text(up)
            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
            lang = st.session_state.get("ui_lang", "zh")

            language_instruction = (
                "Write the output in English."
                if lang == "en"
                else "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¸¦ä»¥ FDA å¯©æŸ¥èˆ‡åˆè¦è§€é»é€²è¡Œèªªæ˜ã€‚"
            )
            prompt = f"""
ä½ æ˜¯ä¸€ä½ FDA æ³•è¦ã€è‡¨åºŠèˆ‡ CMC æ•´åˆåˆ†æå°ˆå®¶ã€‚
{language_instruction}

è«‹é‡å°ä»¥ä¸‹æ–‡ä»¶é€²è¡Œçµæ§‹åŒ–åˆ†æï¼Œæ¶µè“‹ï¼š
- æ–‡ä»¶ç›®çš„èˆ‡é©ç”¨é ˜åŸŸ
- èˆ‡ FDA ç›¸é—œçš„æ³•è¦æˆ–æŒ‡å¼•ï¼ˆå¦‚ 21 CFRã€GxPã€ICH æŒ‡å—ï¼‰ä¹‹é—œè¯
- æ½›åœ¨é¢¨éšªèˆ‡ç¼ºå£
- å»ºè­°è£œå¼·èˆ‡ä¸‹ä¸€æ­¥è¡Œå‹•

[DOCUMENT START]
{text[:100000]}
[DOCUMENT END]
"""
            result = call_llm(
                provider=provider,
                model=model_id,
                system_prompt=system_prompt,
                user_messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
            )
            st.markdown("### Analysis")
            st.markdown(result or "_No output._")
    st.markdown("</div>", unsafe_allow_html=True)


def tab_multi_file_synthesis():
    st.markdown(f"## {t('tab_multi_file')}")
    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)
    files = st.file_uploader(
        "Upload multiple files (PDF/DOCX/TXT/MD)",
        type=["pdf", "docx", "txt", "md"],
        accept_multiple_files=True,
        key="multi_files",
    )
    if files and st.button("Combine & Analyze"):
        with st.spinner("Combining and analyzing filesâ€¦"):
            assembled = []
            for f in files:
                content = extract_text(f)
                assembled.append(
                    f"--- START FILE: {f.name} ---\n{content}\n--- END FILE: {f.name} ---\n"
                )
            combined = "\n".join(assembled)[:150000]

            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
            lang = st.session_state.get("ui_lang", "zh")
            language_instruction = (
                "Write the output in English."
                if lang == "en"
                else "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¸¦å¼·èª¿è·¨æ–‡ä»¶ä¹‹ FDA æ³•è¦è§€é»èˆ‡å·®ç•°ã€‚"
            )

            prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼ FDA å ±è¦èˆ‡è·¨æ–‡ä»¶ç­–ç•¥è©•ä¼°çš„é¡§å•ã€‚

{language_instruction}

ä½ å°‡æ”¶åˆ°å¤šä»½æ–‡ä»¶ï¼Œå·²ä»¥ START/END FILE æ¨™è¨˜å€åˆ†ã€‚
è«‹è¦–å…¶ç‚ºä¸€çµ„ã€ŒçŸ¥è­˜åº«ã€ï¼ŒåŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š

- æ¯”è¼ƒèˆ‡å°ç…§å„æ–‡ä»¶åœ¨æ³•è¦ç«‹å ´ã€è‡¨åºŠè­‰æ“šã€CMCã€é¢¨éšªç®¡ç†ç­‰é¢å‘çš„å·®ç•°èˆ‡ä¸€è‡´æ€§ã€‚
- æ‰¾å‡ºé—œéµè½å·®ã€‚
- ç”¢å‡º Markdown å ±å‘Šï¼ŒåŒ…å«ï¼š
  - Executive Summary
  - Cross-Document Comparisons
  - Key Risks / Gaps
  - FDA å¯©æŸ¥è§€é»ä¸‹çš„å„ªå…ˆé †åºèˆ‡å»ºè­°ä¸‹ä¸€æ­¥

[DOCUMENTS]
{combined}
"""
            result = call_llm(
                provider=provider,
                model=model_id,
                system_prompt=system_prompt,
                user_messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
            )
            st.markdown("### Synthesis Report")
            st.markdown(result or "_No output._")
    st.markdown("</div>", unsafe_allow_html=True)


def tab_smart_replace():
    st.markdown(f"## {t('tab_smart_replace')}")
    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)
    col1, col2 = st.columns(2)
    with col1:
        template_text = st.text_area(
            "Template (with placeholders like [Product Name], [Indication])",
            height=260,
        )
    with col2:
        context_text = st.text_area(
            "Context / Raw Data Source (e.g., protocol, CSR, CMC summary)",
            height=260,
        )

    instructions = st.text_area(
        "Natural language instructions (tone, style, constraints)",
        value="è«‹ä¾ç…§ FDA æ³•è¦èˆ‡ç§‘å­¸åˆç†æ€§å¡«å¯«æ‰€æœ‰æ¬„ä½ï¼Œç¶­æŒå°ˆæ¥­ã€ç²¾ç¢ºä¸”å¯©æŸ¥å‹å–„çš„èªæ°£ã€‚",
    )

    if st.button("Run Smart Replace"):
        provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
        lang = st.session_state.get("ui_lang", "zh")
        language_instruction = (
            "Write the output in English."
            if lang == "en"
            else "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«å®Œæ•´ç¯„æœ¬å…§å®¹ã€‚"
        )

        prompt = f"""
ä½ æ˜¯ä¸€ä½ FDA å ±è¦èˆ‡æ³•å¾‹æ–‡æœ¬æ’°å¯«å°ˆå®¶ã€‚

{language_instruction}

ä¸‹åˆ—ç‚ºä¸€ä»½å«æœ‰å ä½ç¬¦çš„ç¯„æœ¬ï¼š

[TEMPLATE]
{template_text}

ä»¥ä¸‹ç‚ºæœªçµæ§‹åŒ–çš„èƒŒæ™¯è³‡æ–™ï¼š
[CONTEXT]
{context_text}

ä½¿ç”¨è€…èªªæ˜ï¼š
{instructions}

è«‹ä¾æ“š CONTEXT ä¸­è³‡è¨Šï¼š
- è£œé½Šæ‰€æœ‰å ä½ç¬¦
- é¿å…æ†‘ç©ºæé€ é—œéµæ•¸æ“šï¼›è‹¥æ–‡ä»¶æœªæä¾›ï¼Œè«‹ä»¥ã€Œï¼ˆæ–‡ä»¶æœªæä¾›æ˜ç¢ºè³‡è¨Šï¼‰ã€æ¨™ç¤º
- ä»¥ Markdown è¼¸å‡ºå®Œæ•´ä¸”å·²å¡«å¯«å®Œæˆä¹‹ç¯„æœ¬
"""
        with st.spinner("Generating filled templateâ€¦"):
            result = call_llm(
                provider=provider,
                model=model_id,
                system_prompt=system_prompt,
                user_messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
            )
        st.markdown("### Completed Template")
        st.markdown(result or "_No output._")
    st.markdown("</div>", unsafe_allow_html=True)


def tab_ai_note_keeper():
    st.markdown(f"## {t('tab_note_keeper')}")
    st.markdown('<div class="glass-panel">', unsafe_allow_html=True)

    # Sub-tabs inside Note Keeper
    sub1, sub2 = st.tabs(["Magic Transform", "Keyword Coral Keeper"])

    # --- Original Magic Transform ---
    with sub1:
        raw_note = st.text_area("Your raw notes / brain dump", height=240, key="note_raw")
        col1, col2, col3, col4, col5 = st.columns(5)
        action = None
        if col1.button("Format"):
            action = "format"
        if col2.button("Tasks"):
            action = "tasks"
        if col3.button("Fix"):
            action = "fix"
        if col4.button("Summary"):
            action = "summary"
        if col5.button("Expand"):
            action = "expand"

        if action and raw_note.strip():
            provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
            lang = st.session_state.get("ui_lang", "zh")
            language_instruction = (
                "Write the output in English."
                if lang == "en"
                else "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¸¦ç¶­æŒ FDA å ±è¦æˆ–å°ˆæ¥­å¯©æŸ¥æ–‡ä»¶å¸¸è¦‹ä¹‹èªæ°£ã€‚"
            )

            prompt_map = {
                "format": "å°‡é€™äº›ç­†è¨˜æ•´ç†æˆçµæ§‹æ¸…æ¥šçš„ Markdownï¼ˆå«æ¨™é¡Œèˆ‡æ¢åˆ—ï¼‰ï¼Œæ–¹ä¾¿æ—¥å¾Œç”¨æ–¼ FDA æ–‡ä»¶è‰æ“¬ã€‚",
                "tasks": "å¾é€™äº›å…§å®¹ä¸­èƒå–æ‰€æœ‰å¯åŸ·è¡Œä»»å‹™ï¼Œä¸¦ä»¥æ ¸å–æ¸…å–® (- [ ]) æ¢åˆ—ï¼Œè‘—é‡æ–¼ FDA å ±è¦èˆ‡åˆè¦è¡Œå‹•ã€‚",
                "fix": "ä¿®æ­£æ–‡æ³•ã€ç”¨è©èˆ‡é‚è¼¯ï¼Œä½¿å…¶æ›´é©åˆä½œç‚ºå° FDA æˆ–å…§éƒ¨å¯©æŸ¥ä½¿ç”¨çš„å°ˆæ¥­æ–‡å­—ã€‚",
                "summary": "å…ˆçµ¦å‡ºä¸€æ®µç²¾ç°¡ TL;DR æ‘˜è¦ï¼Œå†ä»¥æ¢åˆ—æ–¹å¼æ•´ç†é‡é»èˆ‡é¢¨éšªé …ç›®ã€‚",
                "expand": "å°‡ç°¡çŸ­çš„è¦é»æ“´å¯«æˆè¼ƒå®Œæ•´çš„æ®µè½ï¼Œä¸¦åŠ å…¥ FDA åˆè¦è§€é»æˆ–å¯¦å‹™å»ºè­°ã€‚",
            }
            prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆé–€å”åŠ© FDA å ±è¦åœ˜éšŠæ•´ç†æ€è·¯çš„ã€ŒçŸ¥è­˜ç®¡ç†é¡§å•ã€ã€‚

{language_instruction}

ä½¿ç”¨è€…çš„åŸå§‹ç­†è¨˜å¦‚ä¸‹ï¼š
{raw_note}

ä»»å‹™ï¼š{prompt_map[action]}

è«‹åªè¼¸å‡ºæ•´ç†å¾Œçš„ Markdown ç­†è¨˜ã€‚
"""
            with st.spinner("Transforming notesâ€¦"):
                result = call_llm(
                    provider=provider,
                    model=model_id,
                    system_prompt=system_prompt,
                    user_messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
            st.markdown("### Transformed Notes")
            st.markdown(result or "_No output._")

    # --- New Keyword Coral Keeper ---
    with sub2:
        st.markdown("### ğŸ“‘ é—œéµå­—çŠç‘šæ¨™è¨»ç­†è¨˜ï¼ˆKeyword Coral Keeperï¼‰")
        base_text = st.text_area(
            "è²¼ä¸ŠåŸå§‹æ–‡å­—æˆ– Markdownï¼š",
            height=240,
            key="coral_input_text",
        )
        if st.button("æ•´ç†ä¸¦ä»¥çŠç‘šè‰²æ¨™ç¤ºé—œéµå­—"):
            if not base_text.strip():
                st.warning("è«‹å…ˆè²¼ä¸Šå…§å®¹ã€‚")
            else:
                provider, model_id, max_tokens, temperature, system_prompt = get_llm_config()
                lang = st.session_state.get("ui_lang", "zh")
                language_instruction = (
                    "Write the output in English."
                    if lang == "en"
                    else "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¸¦å°‡é—œéµè©ä»¥ HTML span æ–¹å¼æ¨™ç¤ºã€‚"
                )

                prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ã€Œçµæ§‹åŒ–ç­†è¨˜æ•´ç†å°ˆå®¶ã€ï¼ŒåŒæ™‚ç†Ÿæ‚‰ FDA / ç§‘å­¸ / æŠ€è¡“é ˜åŸŸçš„é—œéµè©ã€‚

ä»»å‹™ï¼š
1. å°‡ä½¿ç”¨è€…è¼¸å…¥çš„æ–‡å­—æˆ– Markdownï¼Œæ•´ç†æˆé‚è¼¯æ¸…æ¥šã€å±¤æ¬¡åˆ†æ˜çš„ Markdown ç­†è¨˜ï¼ˆä½¿ç”¨ #, ##, ###, - ç­‰ï¼‰ã€‚
2. å°‹æ‰¾é—œéµè©ï¼ˆä¾‹å¦‚é‡è¦åè©ã€å°ˆæœ‰åè©ã€é‡è¦æ©Ÿæ§‹ã€é—œéµé¢¨éšªæˆ–å‹•ä½œï¼‰ï¼Œä¸¦ä»¥ä¸‹åˆ— HTML æ ¼å¼æ¨™ç¤ºï¼š
   <span style="color:#FF7F50;font-weight:bold">é—œéµè©</span>
3. å…¶é¤˜æ–‡å­—ä¿æŒä¸€èˆ¬ Markdown æ’ç‰ˆå³å¯ã€‚

{language_instruction}

ä½¿ç”¨è€…åŸå§‹å…§å®¹ï¼š
[NOTE START]
{base_text}
[NOTE END]
"""
                with st.spinner("Organizing and highlighting keywordsâ€¦"):
                    result = call_llm(
                        provider=provider,
                        model=model_id,
                        system_prompt=system_prompt,
                        user_messages=[{"role": "user", "content": prompt}],
                        max_tokens=max_tokens,
                        temperature=temperature,
                    )
                st.session_state["coral_note_md"] = result or ""

        if "coral_note_md" in st.session_state and st.session_state["coral_note_md"]:
            st.markdown("---")
            st.markdown("### âœï¸ å¯ç·¨è¼¯ç­†è¨˜ï¼ˆå«çŠç‘šè‰²é—œéµè©ï¼‰")
            coral_view_mode = st.radio(
                "é¡¯ç¤ºæ¨¡å¼",
                ["Markdown + çŠç‘šè‰²é è¦½", "ç´”æ–‡å­—"],
                horizontal=True,
                key="coral_view_mode",
            )
            coral_text = st.text_area(
                "ç·¨è¼¯ç­†è¨˜å…§å®¹ï¼ˆä¿ç•™ span æ¨™ç±¤å¯ç¶­æŒçŠç‘šè‰²ï¼‰ï¼š",
                value=st.session_state["coral_note_md"],
                height=260,
                key="coral_edit_text",
            )
            st.session_state["coral_note_md"] = coral_text

            if coral_view_mode == "Markdown + çŠç‘šè‰²é è¦½":
                st.markdown("#### é è¦½ï¼ˆå…è¨± HTMLï¼‰")
                st.markdown(coral_text, unsafe_allow_html=True)
            else:
                st.text(coral_text)

            st.markdown("---")
            st.markdown("### ğŸ’¬ é‡å°æ­¤ç­†è¨˜æŒçºŒæå•")
            coral_q = st.text_area("ä½ çš„å•é¡Œ / æŒ‡ä»¤", key="coral_qa_question")
            provider_q, model_q, max_tokens_q, temp_q = limited_model_selector("Gemini 3 Flash")
            coral_answer_view = st.radio(
                "å›ç­”é¡¯ç¤ºç‚º",
                ["Markdown", "ç´”æ–‡å­—"],
                horizontal=True,
                key="coral_answer_view",
            )
            if st.button("å‘æ¨¡å‹æå•ï¼ˆä»¥æ­¤ç­†è¨˜ç‚ºèƒŒæ™¯ï¼‰"):
                if not coral_q.strip():
                    st.warning("è«‹è¼¸å…¥å•é¡Œã€‚")
                else:
                    context = f"""
ä»¥ä¸‹ç‚ºç¶“æ•´ç†ä¸”å«é—œéµè©æ¨™ç¤ºçš„ç­†è¨˜å…§å®¹ï¼ˆåŒ…å« HTML span èˆ‡ Markdownï¼‰ï¼š

[NOTE]
{st.session_state['coral_note_md'][:80000]}
"""
                    with st.spinner("Model thinking with noteâ€¦"):
                        answer = call_llm(
                            provider=provider_q,
                            model=model_q,
                            system_prompt="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çŸ¥è­˜ç®¡ç†é¡§å•ï¼Œè«‹å–„ç”¨ç­†è¨˜å…§å®¹å›ç­”å•é¡Œã€‚",
                            user_messages=[
                                {"role": "user", "content": context},
                                {"role": "user", "content": coral_q},
                            ],
                            max_tokens=max_tokens_q,
                            temperature=temp_q,
                        )
                    if coral_answer_view == "Markdown":
                        st.markdown(answer or "_No answer produced._")
                    else:
                        st.text(answer or "_No answer produced._")

    st.markdown("</div>", unsafe_allow_html=True)


# =========================
#  Main
# =========================

def main():
    st.set_page_config(
        page_title="AuditFlow AI Â· Masterpiece Edition (FDA)",
        layout="wide",
    )

    # Init session defaults
    if "ui_lang" not in st.session_state:
        st.session_state.ui_lang = "zh"
    if "dark_mode" not in st.session_state:
        st.session_state.dark_mode = True
    if "artist_style_key" not in st.session_state:
        st.session_state.artist_style_key = ARTIST_STYLES[0].key

    # Load agents
    agents = load_agents()

    # Sidebar global controls
    with st.sidebar:
        st.markdown("## ğŸŒ Global Settings")
        lang_label = st.radio("Language / èªè¨€", ["English", "ç¹é«”ä¸­æ–‡"], key="lang_radio")
        st.session_state.ui_lang = "en" if lang_label == "English" else "zh"

        dark_mode = st.toggle("Dark mode", value=st.session_state.dark_mode, key="dark_mode_toggle")
        st.session_state.dark_mode = dark_mode

        active_style = style_selector_ui()
        render_api_key_inputs()
        selected_agent = agent_selector_ui(agents)
        render_llm_controls()

    # Apply painter theme
    apply_theme(active_style, st.session_state.dark_mode)

    # Header
    st.markdown(f"<h1 class='accent-title'>{t('app_title')}</h1>", unsafe_allow_html=True)
    st.markdown(t("subtitle"))
    if selected_agent:
        st.markdown(
            f"<div class='accent-chip'>ç›®å‰ä»£ç†äººï¼š{selected_agent.get('display_name_zh','')}</div>",
            unsafe_allow_html=True,
        )

    # Tabs (added OCR tab)
    tab0, tab1, tab2, tab3, tab4, tab5 = st.tabs([
        t("tab_ocr_pdf"),
        t("tab_file_transform"),
        t("tab_file_intel"),
        t("tab_multi_file"),
        t("tab_smart_replace"),
        t("tab_note_keeper"),
    ])

    with tab0:
        tab_ocr_pdf_intelligence()
    with tab1:
        tab_file_transform_deep_summary()
    with tab2:
        tab_file_intelligence()
    with tab3:
        tab_multi_file_synthesis()
    with tab4:
        tab_smart_replace()
    with tab5:
        tab_ai_note_keeper()


if __name__ == "__main__":
    main()
